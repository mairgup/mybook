\documentclass[10pt,a4paper]{ctexart}

\usepackage{geometry}
\usepackage{amssymb}
\usepackage{mathrsfs}
\geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}
\usepackage{xeCJK}
\usepackage{graphics}
\setCJKmainfont[BoldFont={黑体},ItalicFont={楷体}]{宋体}

\makeatletter %使\section中的内容左对齐
\renewcommand{\section}{\@startsection{section}{1}{0mm}
  {-\baselineskip}{0.5\baselineskip}{\bf\leftline}}
\makeatother

\title{神经网络机器翻译与端到端模型综述}
\author{Graham Neubig（著）, 马尔胡甫（译）}
\date{}

\begin{document}
\maketitle

\section{引言}
在本篇综述中，我们介绍了“神经网络机器翻译模型”，亦被称为“神经网络端到端模型”。
这是一类新型的、强大的应用技术。
业界已经把这些技术应用到了自然语言处理的各项任务当中，并取得了很好的效果。
如果你想对序列数据进行建模，那么，这些技术工具将会给你提供最强有力的技术支持。
阅读本文，读者不需要有神经网络和自然语言处理相关的特殊经验，但要有一定的数学及编程方面的基础。
在本综述中，我们先试图去阐述各种方法背后的直观解释，再通过足够的数学细节来深入并准确理解这些方法，最后以开发实现练习的建议收尾，读者可以通过这些开发实践来评判自己对内容的理解程度。

\subsection{背景知识}
在开始讲具体细节之前，有必要解释一下题目“神经网络机器翻译与端到端模型”中的几个术语。
\textbf{机器翻译}是用来对不同语言文本进行翻译的技术。想象一下科幻电影中出现的万能翻译器，让你能够轻松的与不同语言的其他人交流，或者任何一种在线翻译平台，能让你理解并非自己母语写成的文本内容。
毫无疑问，这种消除语言障碍的潜力，在现实中非常有用，也正因为如此，在电子计算刚开始发展不久，研究人员就开始研究机器翻译技术了。

机器翻译系统的输入，通常被称为\textbf{源语言}，而其输出，则称为\textbf{目标语言}。
因此，机器翻译任务也可以被描述为：将源语言的词序列转换成目标语言的词序列的过程。
机器翻译研究人员的目标，就是寻找一种有效的模型来完成这个转换操作，使得任何两种语言的任何文本都能被准确的相互转换出来。

标题的第二部分，\textbf{端到端模型}，是能够将一个序列映射成另一个序列的一类模型的总称。
显然，该模型包括了机器翻译模型在内，但也涵盖了用于解决如图所示的其他任务的一系列方法模型。
实际上，如果我们把计算机程序看做是“输入一个比特序列，然后输出一个比特序列”这样一种模型，那么我们可以说，任何一段程序其实都可以被看做是能执行某种功能的端到端模型（当然，很多情况下，这都不是表述事情最直观或最自然的方式）。

此处有张图

之所以将机器翻译作为介绍这种更大范畴的端到端模型的代表，有如下几点原因：
\begin{enumerate}
\item 机器翻译是非常有用且被广泛使用的一种端到端模型，且方便我们给出一些非常直观的例子，来看看我们都遇到了哪些问题和怎么解决这些问题的。
\item 机器翻译常常是开发新模型时用到的主要驱动任务，也就是说新模型通常先在机器翻译任务中进行调教，再被应用到其他任务中。
\item 然而，也有一些机器翻译从其他任务中学习经验的情况，介绍这些任务本身也有助于理解机器翻译任务。
\end{enumerate}

\subsection{本文结构}
本篇综述第二节内容将给出机器翻译所使用的统计技术背后的基本数学定义。
剩余的篇幅将按照复杂度的递增顺序介绍相关技术模型，直到引出注意力模型（attentional model），该模型是当前业界公认的最好模型。

首先，第三节到第六节将关注点放在\textbf{语言模型}上，该模型是用来计算目标语言词序列的概率值。
这些模型没有完成翻译或序列转换的能力，但能为理解端到端模型提供很好的预备知识。

\begin{itemize}
\item 第三节将介绍$n$\textbf{元语言模型}，这是一种简单的模型，可以通过对词串在语料中的计数情况来计算其概率值。本节还将介绍一种度量语言模型好坏的方法--\textbf{困惑度}。
\item 第四节将介绍\textbf{对数线性语言模型}，该模型用来在给定上文特征的情况下计算下一个词出现的概率值。本节还将介绍如何让计算机通过\textbf{随机梯度下降}方法来学习该语言模型的参数，即，通过计算梯度来一点一点地更新参数，使得训练语料的似然极大化。
\item 第五节将介绍\textbf{神经网络}的概念，相比于前面的对数线性模型，该模型能让我们更好的利用更多的特征信息，并能达到更高的模型精度。本节还介绍了\textbf{前向神经网络语言模型}，该模型可以基于几个上文词条计算出下一个词出现的概率值。
\item 第六节将介绍\textbf{循环神经网络}，这是一类神经网络模型的统称，这类模型的一大优点在于，具有能够记录相隔一段距离的信息的能力。从此引出了\textbf{循环神经网络语言模型}，该模型能够处理远距离依赖问题，这一点在语言模型或其他序列问题建模过程中非常有用。
\end{itemize}

最后，第七第八两小节，将介绍能够完成机器翻译或其他任务的端到端模型。

\begin{itemize}
\item 第七节将介绍\textbf{编码-解码}模型，该模型用一个源端循环神经网络将源语言词序列\textit{编码}为一个向量表示，用另一个循环神经网络模型来对这个向量表示进行\textit{解码}，进而得到输出的句子。本节还将介绍基于该模型来生成输出序列的\textbf{搜索算法}。
\item 第八节将介绍\textbf{注意力（attention）}机制，一种能让模型在产生翻译结果的过程中将关注点放在输入序列的不同片段上的方法。这种机制，能够有效且直观的表示一个句子，且效果上要比其他的简单编码-解码模型要好。
\end{itemize}

\section{统计机器翻译预备知识}
在讲述任何具体模型之前，我们先来看看\textbf{统计机器翻译（SMT）}的整体架构是怎样形式化表述的。

我们这样来定义机器翻译任务：将一个源语言句子$F = f_1,...,f_J = f_1^{|F|}$翻译为目标语言的句子$E = e_1,...,e_I = e_1^{|E|}$。如此一来，任何翻译系统都可以形式化地定义为一个函数
\[
  \hat{E} = mt(F),
\]
这个函数在给定输入$F$的情况下返回一个翻译结果$\hat{E}$。

\textbf{统计机器翻译}系统所做的事情，其实就是定义了一个给定$F$时$E$的概率模型$P(E|F;\theta)$，翻译的过程就是找到一个目标语言的句子，使得这个概率最大化：
\[
 \hat{E} = \mathop{\arg\max}_{E} P(E|F;\theta),
\]
其中，$\theta$是这个概率模型的具体参数。这个参数$\theta$的值是从\textbf{双语对齐语料库}上学习出来的，这个双语对齐语料库的形式，具体可以理解为源语言的句子和其对应的目标语言的句子的一一对应。
在这样的框架下，为了得到一个好的翻译模型，我们会遇到如下几个主要的问题需要着手解决：
\begin{enumerate}
\item[\textbf{建模}:] 首先，需要定义我们的概率模型$P(E | F;\theta)$的具体形式。该模型需要哪些参数，以及如何来衡量这些参数对概率分布的拟合度。
\item[\textbf{学习}:] 其次，我们需要一种能从训练语料中学习参数$\theta$的方法。
\item[\textbf{搜索}:] 最后，我们需要解决“如何搜索最可能的目标句子”的问题（即解决公式中的''argmax"）。这个搜索最优目标串的过程一般被称为\textbf{解码过程}。
\end{enumerate}

本篇综述剩余的内容，将专注于解决这几个问题而展开。

\section{$n$-gram 语言模型}
我们知道，一个统计机器翻译系统要做的任务，是对给定源语言句子$F$的前提下的所有目标语言句子$E$的概率分布$P(E | F)$进行建模。
在本节中，我们先回退一步，先构造\textbf{语言模型}，来针对目标句子的概率分布$P(E)$建模。
这个模型，在实际应用当中，通常用来解决如下两个问题。
\begin{enumerate}
\item[\textbf{衡量流畅度}:] 给定一个句子$E$，语言模型可以告诉我们：这句话是否是真实的、自然的目标语言句子？如果我们能够训练一个模型来回答这个问题，我们就可以用它来评价一个系统自动生成的句子的流畅度，来提升翻译结果的质量。当然，也可以用这个模型来对人书写或说出的句子进行评价，以达到句法检查或纠错的目的。
\item[\textbf{文本生成}:] 语言模型可以用来自动生成文本，具体的做法是，根据目标概率分布$P(E)$随机的采样出一个句子$E'$。用语言模型随机的生成采样文本，本身就是一件非常有趣的事，我们可以看到模型“认为”的自然句子是什么样子的，但它在后面将要介绍的神经网络翻译模型的使用场景下更有实际意义。
\end{enumerate}
在接下来的小节中，我们将介绍一些常见的方法，来计算这个概率分布函数$P(E)$。

\subsection{基于词的概率计算}
如上所述，我们的兴趣点在于计算一句话$E=e_1^T$的概率值。我们可以将这个任务形式化地定义为
\[
  P(E) = P(|E| = T,e_1^T),
\]
包含$T$个词的的句子$E$的联合概率分布，具体地，句子中第一个词记作$e_1$，第二个词记作$e_2$，...最后一个词为$e_T$。
不幸的是，由于句子的长度$T$并不是一个定值，并且词与词的组合方式呈现出组合爆炸的趋势，因此，我们很难对这样一个概率分布函数进行直接的建模。

这里有个图

为了让事情变得可操作一些，我们常常把一个句子的概率值重定义为句子中每个词的概率乘积。
这样，可以利用一个联合概率--例如$P(e_1,e_2,e_3)$--可以通过其中每个元素的概率乘起来来计算得出，也就是说$P(e_1,e_2,e_3)=P(e_1)P(e_2|e_1)P(e_3|e_1,e_2)$。

图2中给出了用这种方法来得到例句“她 回 家”的概率值得计算过程。
这里，除了句子中真实的词条之外，我们引入了一个\textit{句末符}（“</s>”），用来表征一个句子在此处终结。
从一步一步计算概率的过程中可以看出，我们顺次计算了“她”出现在句首的概率，“回”紧跟在句首的“她”后面的概率，“家”紧跟在上文“她 回”之后的概率，以及句末符“</s>”紧跟在“她 回 家”之后的概率。
更一般地，我们可以用下面这个表达式来表示：
\[
 P(E) = \prod_{t=1}^{T+1} P(e_t|e_1^{t-1}),
\]
其中$e_{T+1}=</s>$。
说回到句末符</s>，我们之所以引入该符号，是为了让它来告诉我们一个句子何时终止。
换句话说，通过检查句末符</s>的位置，我们可以来决定$|E|=T$个词的原始语言模型联合概率。
还拿刚才的例子来说，当我们发现句子中第4个词是</s>时，我们就知道了这个句子的长度为3。

我们有了上式4中的计算表达式后，语言模型建模的问题，就变成了给定前驱词序列$e_1^{t-1}$的情况下计算当前词$e_t$的概率分布$P(e_t|e_1^{t-1})$的问题。
这样，相比于直接计算整个句子的概率而言，这样的建模方式，可操作性更强了，因为我们现在有了一个固定的词条集合来对其概率分布进行计算。
接下来的几个小节，将介绍几种方法，来计算这个概率分布函数。


\subsection{基于计数的$n$-gram语言模型}
第一种计算概率分布的方法很简单：先准备一个训练语料，我们可以在该语料里对词串计数，即统计某个特定词串在语料中出现的次数，然后除以其上文在语料中出现的次数。这个简单的方法，可以形式化的表述为如下的表达式，图中给出了一个实例。
\[
  P_{ML}(e_t|e_1^{t-1})=\frac{C_{prefix}(e_1^t)}{C_{prefix}(e_1^{t-1})}.
\]
这里，$C_{prefix}(\cdot)$是该特定的词串在语料中出现在句首的次数统计。这个统计方法叫做\textbf{最大似然估计}（MLE，详情见后续内容），这种方法既简单，又能保证得到的概率模型能对训练语料中见过的句子赋予高概率值。

然而，假设有一个我们从未见过的一句话，我们想要用这个模型给这个句子赋予概率值。
举个例子，在上面例子中的训练语料的基础上，如果我们想计算“i am from utah .”这句话的概率值。
这个句子和我们在训练语料中见过的句子及其相似，但不幸的是，词串“i am from utah”并没有在训练语料里出现过，即$C_{prefix}(i,am,from,utah)=0$，概率值$P(e_4=utah|e_1=i,e_2=am,e_3=from)$就变成零了，因此，通过公式5计算得到的这句话的概率值也变成了零。
事实上，这个语言模型，会给所有未在训练语料里出现过的句子，都赋予零概率，也就是说，这个模型失去了判断“系统生成的新句子是否是自然的句子”的能力，或者说让系统失去了生成新句子的能力，因此，这个模型对我们而言，也就失去了使用价值。

为了解决这个问题，我们采取两项措施。
第一，我们不再从句首开始计算概率值，而是取一个固定的上文窗口，在计算概率时只考虑出现在这个窗口范围内的词条，用这种方式来近似的计算真实的概率值。如果我们将上文的范围限定为$n-1$个前驱词，那么这个概率值计算公式近似为
\[
  P(e_t|e_1^{t-1}) \approx P_{ML}(e_t|e_{t-n+1}^{t-1}).
\]
基于这个假设的模型，我们称之为\textbf{$n$-gram语言模型}。具体地，当$n=1$时我们称之为一元模型，当$n=2$时称为二元模型，$n=3$时称为三元模型，$n \geq 4$时称为四元模型、五元模型，以此类推。

$n$-gram语言模型的参数$\theta$，包含了所有的基于上文$n-1$个词时下一个词出现的概率值，即：
\[
  \theta_{t-n+1}^{t} = P(e_t | e_{t-n+1}^{t-1}),
\]
为了训练一个$n$-gram语言模型，我们需要从训练数据里学习这个参数。
简单地，这些参数可以用极大似然估计方法来计算得到，计算公式如下：
\[
  \theta_{t-n+1}^{t} = P_{ML}(e_t | e_{t-n+1}^{t-1}) = \frac{c(e_{t-n+1}^{t})}{c(e_{t-n+1}^{t-1})},
\]
其中，$c(\cdot)$是词串在语料中任意位置出现的次数。
有时，我们会遇到$e_{t-n+1}$中下标$t-n+1<0$的情况，这种情况下，我们默认$e_{t-n+1}=<s>$，其中<s>是一个特殊的\textit{句首符}。

如果我们回到前面的例子中，且设$n=2$，我们可以看到，虽然词串“i am from utah .”在训练语料里并没有出现过，但是，“i am”，“am from”，“from utah”，“utah .”，以及“. </s>”这些词串都在语料里出现过，因此，我们可以将它们的概率值乘起来，来给这个句子计算出一个非零的概率值。

然而，我们还有一个问题：如果遇到训练语料里没有出现过的双词串，我们该如何处理？碰到这种情况时，这个没有出现过的双词串的概率值依旧是零，导致我们计算整句概率时也得到零概率。
$n$-gram语言模型通过\textbf{平滑}概率值来解决这个问题，即通过将不同的$n$元极大似然估计值一并进行考虑。
具体的，我们来看看一元和二元概率的平滑的情况，我们假设一个模型会以如下的形式来计算概率值：
\[
 P(e_t|e_{t-n+1}^{t-1}) = (1 - \alpha)P_{ML}(e_t|e_{t-1}) + \alpha P_{ML}(e_t),
\]
其中，$\alpha$是一个权值参数，用来制定我们给一元概率分配多大的权重。只要我们设$\alpha > 0$，不管上文如何，词表中的所有词条都会被赋予一定的非零概率值。我们称这种方法为\textbf{概率插值}，这是一种能让概率模型更加鲁棒标准做法，对那些低频的长尾词更有效。

如果我们想考虑更长的上文，比如$n=3$，$n=4$，$n=5$，或者更长的上文，我们可以如下的形式递归地定义我们的概率插值方法：
\[
 P(e_t|e_{t-m+1}^{t-1}) = (1 - \alpha_{m})P_{ML}(e_t|e_{t-m+1}^{t-1}) + \alpha_{m}P(e_t|e_{t-m+2}^{t-1}).
\]
上述公式右边第一项，是$m$元概率模型的极大似然估计，而第二项，是1元到$m-1$元概率模型的插值。

当然，还有很多更复杂的平滑方法，但都超出了本文要讨论的范畴，不过可以在【这里】找到很好的相关平滑方法的总结。

\begin{enumerate}
\item[\textbf{上下文相关平滑系数}:] 不像上面所述的那样使用一个固定的$\alpha$，我们改用和上文相关的插值系数：$\alpha_{t-m+1}^{t-1}$。这样一来，当有足够数目的训练样本来准确地学习到某个高阶$n$元概率参数时，概率模型将给这个高阶的$n$元概率赋予更大的权值，当训练数据样本很少时，才回退到低阶$n$元概率上来。这个上下文相关平滑系数，可以通过启发式方法来设定，亦或从训练数据中学习而来。
\item[\textbf{回退模型}:] 在公式9中，我们对词表$V$上的两个概率分布进行了插值。而\textbf{回退模型}与其不同，只有当高阶$n$元概率值为零时，才用低阶概率分布来计算词串的概率值。回退模型的表达能力更强，相比于插值方法也更复杂，论文【】指出这两种方法得到的效果也是差不多的。
\item[\textbf{修改模型}:] 使用一种和$P_{ML}$不同的概率分布也是可行的。在计算概率值之前，从计数结果中“扣除”一个固定的常数，这种方法叫做\textbf{discounting}。当然，也可以修改低阶分布的计数值来反映这样一个事实，只有当高阶分布参数没有被训练语料覆盖到时才会降阶到低阶分布。
\end{enumerate}
目前，\textbf{Modified Kneser-Ney平滑}（MKN），是$n$-gram语言模型常用的一种标准且有效的平滑方法。MKN平滑使用了上下文有关平滑系数，折现法，以及修改版的低阶概率分布，来确保精确的概率估计。

\subsection{语言模型的评价方式}
当我们训练得到一个语言模型后，我们想要对其进行测试，来看看它是否能达到如期效果。
跟很多其他机器学习模型一样，我们评测语言模型时，也需要准备如下三个数据集：
\begin{enumerate}
\item[训练集:] 用来训练模型的参数$\theta$。
\item[开发集:] 用来对不同参数的模型进行选择，或者用来调教模型的\textbf{超参数}。上文所述模型的超参数，包括上文长度$n$以及所使用的平滑方法。
\item[测试集:] 用来评价最终模型的准确率。
\end{enumerate}

对语言模型而言，我们想知道的，无非是该模型是否是语言的准确模型，而我们对这个准确性也有很多定义的方法。
最直观的准确性定义，是开发集或测试集上模型的\textbf{似然}。
参数$\theta$在这些数据集上的似然，等价于该参数下模型赋予这些数据的概率值。
比如，我们有一个测试集$\xi_{test}$，这个概率值就是：
\[
 P(\xi_{test};\theta).
\]
我们一般假设这个数据中包含了一组互相独立的句子或文档$E$，我们就得到了：
\[
 P(\xi_{test};\theta) = \prod_{E \in \xi_{test}} P(E;\theta).
\]

另一个常见的评价方法是使用\textbf{对数似然}
\[
 \log P(\xi_{test};\theta) = \sum_{E \in \xi_{test}} \log P(E;\theta).
\]
使用对数似然有这么几方面的原因。
首先，语言模型赋予任何一个句子的概率值都可能是一个极小的数值，这些极小的概率值乘起来，就会变得无限小，会在标准的计算机硬件上导致精度溢出问题。
其次，在数学上，通常在对数空间进行相关运算更方便。比如，在使用基于梯度的方法来优化参数时（将在下一节介绍），对求和公式13中的参数求偏导数，比对求积公式11中的参数求偏导数要来的更容易一些。

通常，我们还需要对对数似然值除以测试语料中出现的词数
\[
 length(\xi_{test}) = \sum_{E \in \xi_{test}} |E|.
\]
这样一来，我们就能更方便的比较和对比不同长度语料上的评测结果了。

最终，我们得到了语言模型精确度的最常用评价指标：\textbf{困惑度}，即每个词的平均负对数似然的指数：
\[
 ppl(\xi_{test};\theta) = e^{-(\log P(\xi_{test};\theta))/length(\xi_{test})}.
\]
对困惑度的一个直观的解释是：“模型对自己的决策有多困惑？”更准确的讲，它表达的是这样一个值：“在生成一个句子中某个位置的词时，如果我们从语言模型计算的概率分布中随机的挑选某个词来填这个空，我们平均需要挑选多少次，才能得到那个正确的词？”
我们之所以在研究论文里经常看到困惑度的使用，是因为困惑度计算出来的数值一般都比较大，这样，能让人肉眼就能区分出不同模型的好坏。

\subsection{未登录词问题}
最后，还有一件重要的事情需要牢记，那就是评测集$\xi_{test}$中的某些词可能在训练集$\xi_{train}$里从未出现过。
这些没有出现过的词，我们称为\textbf{未登录词}，我们必须要用某种方法来解决这些词的问题。
语言模型中常见的处理方法如下：
\begin{enumerate}
\item[] \textbf{假设词典是封闭集}：有时候，我们可以假设测试集里不会有新词。例如，当我们计算ASCII字符上的语言模型时，认为所有的字符都在训练集里见过这个假设是比较合理的。近似的，在一些语音识别系统中，通常都会简单地给那些未在训练数据里出现过的词赋予零概率，意思就是是这些词不会被识别。
\item[] \textbf{与未登录词分布插值}：如公式10所述，我们可以对高阶和低阶分布进行插值。在遇到未登录词时，我们可以认为这是“0”阶分布，可以将1-gram概率定义为一元概率分布和未登录词分布的插值\[ P(e_t) = (1 - \alpha_1)P_{ML}(e_t) + \alpha_1 P_{unk}(e_t).\]这里，$P_{unk}$是一个概率分布，且为所有的词$V_{all}$都赋予一个概率值，而不只是针对训练语料里学到的词表$V$。具体地，可以这么操作，训练一个字符级语言模型，能够“拼写”出不在我们词表中的未登录词。或者，我们可以大概估计一下我们所进行建模的语言的词表大小$|V_{all}|$，其中$|V_{all}| > |V|$，并将$P_{unk}$定义为全部词表上的一个均匀分布：$P_{unk}(e_t) = 1 / {V_{all}}$。
\item[] \textbf{添加<unk>符号}：作为最后一种处理未登录词问题的方法，我们可以将训练集$\xi_{train}$里的某些词条删去，将其替换为表示未登录词的特殊符号<unk>。一种常见的做法是，将训练集中的只出现了一次的\textbf{单次词}替换掉。通过这种方法，我们可以具体地预测在何种上文下我们可能会遇到未登录词，而不是用上面讲的插值法来预测具体的某个词。即使我们只是预测了<unk>符，我们也需要估计此处出现的具体的词的概率值，因此，每当在句中位置$i$处预测了<unk>，我们都需要乘上一个概率值$P_{unk}(e_t)$。
\end{enumerate}

\subsection{补充阅读}
如果想阅读$n$-gram语言模型相关的具体文献，论文【】是不二之选。
我们上面介绍的简化版$n$-gram语言模型具有很多不足之处，这篇论文给出了解决这些不足的一系列方法，并且其介绍过程非常的易于理解。

还有一些$n$-gram概率模型的扩展问题，也许对有兴趣的读者有帮助。
\begin{enumerate}
\item[] \textbf{大规模语言模型}：语言模型是很多商业工具的组成部件，在这些工具中的语言模型，常常是在大规模的网页文本数据上训练而来的。为了处理这种海量的数据集，也有一些针对高效的数据结构的研究，如【】，分布式参数服务器，以及有损压缩算法等。
\item[] \textbf{语言模型自适应}：在很多情况下，我们需要针对某个具体的用户、或者某个具体的领域，单独训练一个语言模型。自适应技术的做法，是先训练一个大而普世的语言模型，然后再让这个普世模型去自动适应目标情景。
\item[] \textbf{基于计数的远距离语言模型}：如上所述，$n$-gram语言模型将上文长度限定为$n-1$，但是在现实中，会有一些词依赖于句子中很远的某个上文词，或者依赖整篇文本前的某些内容。我们在第六节将介绍的循环神经网络语言模型就是用来解决这种远距离依赖问题的，但也有一些非神经网络模型，比如缓存语言模型，主题模型，skip-gram模型。
\item[] \textbf{句法语言模型}：还有一些模型使用了目标句子的句法信息。例如，我们不仅可以基于句子中相邻的几个词来计算条件概率，我们还可以基于句法上“相邻”的几个词来计算条件概率。
\end{enumerate}

%\subsection{练习}

\section{对数线性语言模型}
本节我们将讨论另一种类型的语言模型：\textbf{对数线性语言模型}，相比于上文介绍的基于计数的$n$-gram模型，对数线性语言模型使用了截然不同的方法。

\subsection{形式化定义}
对数线性语言模型和$n$-gram语言模型一样，也是计算给定特定上文$e_{t-n+1}^{t-1}$时一个特定的词$e_t$出现的概率。
然而，对数线性语言模型计算这个概率所用的方法，和基于计数的语言模型相比，有很大的不同。对数线性语言模型计算这个概率值得过程可以分为如下几步进行。

\textbf{计算特征值}：对数线性模型的一个基本概念就是\textbf{特征}。简单来讲，特征就是“上文中对预测下一个词有用的信息”。
形式化地，我们可以这样定义一个特征函数$\phi (e_{t-n+1}^{t-1})$，这个函数把上文当做其输入，然后输出一个实值得\textbf{特征向量$x$}$\in \mathbb{R}^N$，用来描述这个上文的$N$个不同特征。

拿我们上一节介绍的二元模型来举例，我们知道“前一个词的编码”对预测下一个词是很有用的信息。如果我们要用一个实值向量来表示这个“前一个词的编码”，我们可以假设词表$V$中的每一个词都有一个与之对应的编号$j$，$1 \leq j \leq |V|$。
然后，我们定义一个特征函数$\phi (e_{t-n+1}^t)$，让其返回一个特征向量$x = \mathbb{R}^{|V|}$，如果$e_{t-1}=j$，就让向量中的第$j$个元素等于1，而其余的元素都设为0。这种向量，一般被称作\textbf{词向量（one-hot vector）}，在图4中给出了一个样例。
为了后面表述方便，我们也定义了一个函数$onehot(i)$来返回一个向量，其中只有第$i$个元素为1而其余元素均为0（假设这个向量的长度是适中的）。

这里有个图。

当然，我们没有必要受限于只考虑一个上文词。我们也可以分别计算出$e_{t-1}$和$e_{t-2}$的向量表示，然后将它们首尾相接拼起来，这样就能建构一个同时考虑了两个上文词的模型。
事实上，我们还能想到很多其他种类的特征函数（4.4小节将介绍更多），而能够弹性的定义这些特征的能力，就是对数线性语言模型相对于标准的$n$-gram语言模型的一大优势所在。

\textbf{计算得分}：一旦我们有了特征向量，我们就想用这些特征来预测输出词表$V$的概率分布。为了达到这个目的，我们计算得到一个得分向量$s \in \mathbb{R}^{|V|}$，其中的每一维数值对应着词表中的一个词似然值，得分越大的词出现的概率也越大。
我们的模型参数$\theta$具体来说就有两个：一个是\textbf{偏执向量}$b \in \mathbb{R}^{|V|}$，用来表征词表中的一个词大体上的似然，还有一个是\textbf{权值向量}$W = \mathbb{R}^{|V| \times N}$，用来表征特征值和得分结果之间的关系。
到此，用来计算某个上文的得分公式如下：
\[
 \textbf{s} = \textbf{Wx} + \textbf{b}.
\]

在此值得一提的，是作为特殊情况，one-hot向量或者其他的大部分元素为0的\textit{稀疏向量}。
因为此，我们也可以把公式17看成是和其数学上等价的形式，这样能让计算过程效率更高。
具体来说，我们不再用一个大的特征向量和大的权值矩阵乘起来，我们可以像下面这样，将权值矩阵的所有由特征向量\textit{激活}的列相加：
\[
 \textbf{s} = \sum_{\{j:x_j \neq 0\}} W_{\cdot,j}x_{j} + \textbf{b},
\]
其中，$W_{\cdot,j}$是权值矩阵$W$的第$j$列。这样可以把计算得分的过程理解为“在权值矩阵中查询特征向量中激活元素所对应的列向量，然后将它们相加”，而不把这个过程看做是矩阵运算。
图5中给出了一个例子，就是用这种方式来处理两个特征函数的情况（一个是前一个词，另一个是前一个词前面那个词）。


\textbf{计算概率}：值得一提的是，得分\textbf{$s$}是一组实数，而不是概率值，这些实数可以为负数或者是大于1的数，而且也没有加和为1的限制。因此，我们需要让这组实数通过这样一个函数来进行如下变形：
\[
 p_{j} = \frac{exp(sj)}{\sum_{\bar{j}}}exp(s\bar{j}).
\]
通过取对数，然后除以词表中所有词对应的得分值的对数和，这样，这些得分值就可以变成一个概率了，即在0到1之间且加和为1。

这个函数被称为\textbf{softmax}函数，一般以下面的向量形式来表示：
\[
 \textbf{p} = softmax(\textbf{s}).
\]
将这个函数运用在上一节中计算得到的得分向量上，我们就得到了一种从特征到语言模型概率的计算方法。

\subsection{学习模型参数}
现在，只剩下最后一个问题了，即如何获取并学习参数$\theta$的值，包括权值矩阵$W$和偏执向量$b$。
简单地讲，我们所使用的方法，就是迭代地寻找更符合训练语料的参数值。

为了达到优化参数的目的，我们使用机器学习中标准的方法。
首先，我们定义一个\textbf{损失函数}$\mathcal{L}(\cdot)$，用来衡量我们的模型在训练语料上表现的不好的程度。
在多数情况中，我们定义这个损失值等于\textbf{负对数似然}：
\[
 \mathcal{L}( \xi_{train},\theta ) = - \log P(\xi_{train} | \theta ) = - \sum_{E \in \xi_{train}} \log P(E |\theta).
\]
我们假设可以定义如下的基于词为粒度的损失函数：
\[
 \mathcal{L}(e_{t-n+1}^{t},\theta) = \log P(e_t | e_{t-n+1}^{t-1}).
\]

下一步，我们通过优化参数来降低这个损失。有很多优化参数的方法，在最近几年，一个较常用的方法是\textbf{随机梯度下降法（SGD）}。
随机梯度下降法是一种迭代地优化参数的方法，我们随机的选择一个单词$e_t$（或者mini-batch，将在第5节进行介绍），然后来提升$e_t$的似然。为了达到这个目的，我们先对损失函数的参数$\theta$中的每个特征参数计算偏导数：
\[
 \frac{d\mathcal{L}(e_{t-n+1}^{t},\theta)}{d\theta}.
\]
然后，我们可以用这个信息，往降低目标函数损失的方向迈出一小步
\[
 \theta \leftarrow \theta - \eta \frac{d\mathcal{L}(e_{t-n+1}^{t},\theta)}{d\theta}.
\]
这里的$\eta$是我们的\textbf{学习率}，用来具体的决定每次参数更新时的参数更新幅度。如此，我们可以找到能够在训练数据上降低损失或者说提升似然的模型参数。

这个简易版的随机梯度下降方法不仅简单，而且也是一个适合学习大规模系统参数的有效方法之一。
当然，为了让训练过程保持稳定，还有一些问题需要考虑。
\begin{enumerate}
\item[] \textbf{调整学习率}：随机梯度下降方法要求选取$\eta$时格外小心：如果$\eta$过大，训练过程会不稳定且会发散，如果$\eta$过小，训练过程会非常缓慢且会落到不好的局部最优中。解决这个问题的方法之一是\textbf{学习率衰减}，在开始训练时使用较大的学习率，随着学习的进程不断减小学习率，直到训练结束。下文中还会简单介绍其他更复杂的方法。
\item[] \textbf{提前停止}：我们知道，我们在训练时普遍会选择一个开放集，来评测模型在这个数据集上的似然，当模型在开发集上达到最优似然值时保存模型。这样做，可以有效防止模型过拟合于训练数据而失去泛化能力。另一种防止过拟合和保持训练过程平滑收敛的方法是这样的，在开发集上评测对数似然，当这个对数似然不再提升或者开始变差，就降低学习率。
\item[] \textbf{乱序学习}：随机梯度下降方法在处理训练数据时的一个特点，就是每次训练都使用一个样本。因为简单和有效，这样做其实是很好的，但这样做也会产生一些问题，尤其是当训练样本的排序有一定的偏执的时候。例如，如果我们的训练数据是新闻语料，而且开始部分是时政新闻、然后是体育新闻，再然后是娱乐新闻，这样的顺序，很有可能在训练过程的后段，我们的模型会遇到几百甚至几千条连续的娱乐新闻样本，从而导致参数将偏向一个更偏爱刚刚见过的训练样本的特征空间。为了防止这个问题，通常（强烈推荐）在每一轮训练模型参数之前，都要随机的打乱训练样本的出现顺序。
\end{enumerate}

当然，学术界也提出了一些其他参数更新方法，同样可以达到使训练过程更稳定和有效的目的。下面列了一些具有代表性的方法：
\begin{enumerate}
\item[] \textbf{惯性随机梯度下降}：与每次在当前梯度方向上走一小步的过程不同，惯性随机梯度下降方法所使用的梯度是历史上所有梯度的平均值。这样可以减轻简单的随机梯度下降方法的“抖动”倾向，让优化过程能够更平滑的拟合参数空间。
\item[] \textbf{AdaGrad}：AdaGrad关注于这样一个事实，一些参数的更新频率比其他参数要高很多。例如，在上面的模型中，上文中出现的低频词所对应的权值矩阵中的列向量，在训练过程中只会被更新几次而已，而偏执向量$\textbf{b}$会在每一个训练样本上都进行更新。基于此，AdaGrad会针对每一个参数具体地动态调整学习率$\eta$的值，频繁更新的参数，如偏执向量$\textbf{b}$，每次会进行很小的更新，而不频繁更新的参数，如权值矩阵$W$，则每次做较大幅度的更新。
\item[] \textbf{Adam}：Adam是另一种为每一个参数单独计算学习率的方法。通过记录每个参数的历史梯度的平均值和方差，使用类似于前面两种方法来计算梯度。由于能显著加快很多数据集上的模型训练收敛速度，缩短了实验迭代的周期，都使得Adam成为目前非常受欢迎的优化方法之一。然而，该方法也因趋向于过拟合而著称，因此，如果高性能是直观重要的条件，那么，相较于标准的随机梯度下降方法而言，Adam方法的使用需要更加小心谨慎才行。
\end{enumerate}
论文【】是对这几个不同的方法的一个很好综述，提供了很多公式和说明，并指出了使用随机优化参数方法时的一些注意事项。

\subsection{对数线性模型的偏导数}
现在，揭晓谜底的最后一块拼图，只剩下如何计算损失函数的每个参数的偏导数了。
为此，我们先从头到尾梳理一下损失函数的计算过程：
\[
 \textbf{x} = \phi (e_{t-m+1}^{t-1})
\]
\[
  \textbf{s} = \sum_{j:x_j \neq 0} W_{\cdot,j}x_j + \textbf{b}
\]
\[
 \textbf{p} = softmax(\textbf{s})
\]
\[
 \mathcal{L} = - \log \textbf{p}_{e_t}
\]
如此一来，可以用链式法则来计算
\[
 \frac{d\mathcal{L}(e_{t-n+1}^t,W,b)}{d\textbf{b}} = \frac{d\mathcal{L}}{d\textbf{p}}\frac{d\textbf{p}}{d\textbf{s}}\frac{d\textbf{s}}{d\textbf{b}}
\]
\[
 \frac{d\mathcal{L}(e_{t-n+1}^t,W,b)}{d\textbf{W}_{\cdot,j}} = \frac{d\mathcal{L}}{d\textbf{p}}\frac{d\textbf{p}}{d\textbf{s}}\frac{d\textbf{s}}{d\textbf{W}_{\cdot,j}}
\]
我们可以发现，损失函数对偏执向量和权值矩阵的每一列的偏导数是：
\[
 \frac{d\mathcal{L}(e_{t-n+1}^t,W,b)}{d\textbf{b}} = \textbf{p} - onehot(e_t)
\]
\[
 \frac{d\mathcal{L}(e_{t-n+1}^t,W,b)}{d\textbf{W}_{\cdot,j}} = x_j (\textbf{p} - onehot(e_t))
\]
确认这些公式的准确性就留给读者自己完成了。提示：在计算这个偏导数时，相比于直接操作$\textbf{p}$来说，操作$\log \textbf{p}$会更容易一些。

\subsection{语言模型的其他特征}
对数线性模型之所以好的原因，是因为该模型能让我们更灵活的设计特征，只要我们觉得这些特征对预测下一个词有用。例如，这样的特征包括但不限于：
\begin{enumerate}
\item[] \textbf{上文词特征}：如上例所示，我们可以用$e_{t-1}$或者$e_{t-2}$的编码作为上文词特征。
\item[] \textbf{上文类特征}：上文中的词可以按相互之间的相似度归为不同的类中，与其用onehot向量来表示每个特定的词，我们可以用onehot向量来表示每个词所属的类别。如此，同一个类中的词条可以共享统计信息，从而使得模型有更好的泛化能力。
\item[] \textbf{上文后缀特征}：也许我们想要一个特征函数，每当前一个词以“...ing”结尾或其他类似的后缀结尾时，这个特征函数就会被激发。这样，模型能够学到更加泛化的构词法相关特征，比如哪些词倾向于出现在进行时态动词的后面等。
\item[] \textbf{词袋子特征}：我们也可以用所有出现在句子上文中的词作为特征，而不是只用前$n$个词。这相当于计算前一个句子中的每一个词的onehot向量，然后将它们加起来，而不是拼接在一起。这种做法，将损失掉词条所在的句中位置信息，但能够捕获哪些词倾向于在同一个句子或同一个文档中共现的信息。
\end{enumerate}

当然，同时使用不同的特征也是完全可行的（比如，$e_{t-1}$是一个独立的词，$e_{t-2}$也是一个独立的词）。
这是构造更具解释性的特征集的方法之一，当然也有其短板，最明显的就是这样做会无限的扩大特征空间的范围。
我们在第5.1小节讨论这些特征的具体细节。

\subsection{扩展阅读}
本节所介绍的语言模型，基本可以理解为$n$-gram语言模型的特征化版本。
当然，还有不少其他的线性特征化模型，包括：
\begin{enumerate}
\item[] \textbf{整句语言模型}：这些模型预测整句话的概率分布然后进行归一化，而不是一个词一个词的进行预测。可以通过引入一些其他特征来达到这个目的，比如句子的长度的概率分布信息，或者诸如“该句子中是否含有动词”这样的特征。
\item[] \textbf{判别语言模型}：如果我们只是希望语言模型能够判别一个系统输出的句子是好的还是差的，有时候直接按这个目标训练模型会更好，然后用训练得到的模型对系统输出结果进行重排序（re-rank），从而得到更高的准确率。即使我们没有真正意义上的反例样本，我们也可以通过“伪造”反例样本的方法来进行训练，而且能达到训练的目的。
\end{enumerate}

%\subsection{练习}

\section{神经网络和前向神经网络语言模型}
在这一节，我们将介绍基于\textbf{神经网络}的语言模型，一种学习更加复杂的函数的方法，不仅提高了概率预测的准确率，而且减少了人工特征工程的烦恼。

\subsection{特征组合的潜力与问题}
在介绍神经网络的具体技术细节之前，我们先来看看图6中的一个直观例子。从例子中，我们可以看到$e_{t-1}$=“farmers”和$e_t$=“hay”是可搭配的（在句子“farmers grow hay”中），而$e_{t-1}$=“eat”也与之可搭配（在句子“cows eat hay”中）。如果我们使用的对数线性模型中，有一个特征集与$e_{t-1}$相关，而另一个特征集与$e_{t-2}$相关，任何一个特征集都不能把“farmers eat hay”这个不自然的短语排除掉。

这里有个图

我们可以通过一种方法来解决这个问题，即另取一个特征集，来学习每两个词对$e_{t-2},e_{t-1}$的向量表示。
如果这样，上文$e_{t-2}$=“farmers”,$e_{t-1}$=“eat”的向量对于“hay”会赋予一个低得分，从而解决这个问题。
然而，加入这种组合特征有一个主要的缺陷：这样会极大的扩大参数个数，不只需要为每一个词对$e_{i-1},e_i$设置$O(|V|^2)$个参数变量，我们需要对任意三元组$e_{i-2},e_{i-1},e_i$设置$O(|V|^3)$个参数变量。这些数字会让模型占用的内存空间急剧扩大，且如果没有足够的训练样本，这些参数也没办法学的很到位。

由于这种组合特征即重要，又很难学习到，因此，学者提出了一系列方法来使用这些特征，比如\textbf{有核支持向量机}和\textbf{神经网络}。具体到本节的内容，我们将介绍神经网络，这个模型相对更灵活且容易在大数据上进行训练，也是介绍端到端模型迫切需要介绍的内容。

\subsection{神经网络概述}
为了理解神经网络的具体细节，我们先来举一个学习一个函数的例子，这个函数我们没法用前一节介绍的简单线性分类器来学到，这个函数的具体表达为：接收输入$\textbf{x} \in \{-1,1\}^2$然后当$x_1=x_2$时输出$y=1$，否则输出$y=-1$。这个函数对应的图如图7所示。

这里有个图

尝试来解这个方程的第一步，可以定义如下的线性模型来解这个问题：
\[
 y = W\textbf{x} + b.
\]
然而，用于解决手头的问题，这类方程还没有足够的能力。

因此，我们转而使用稍加复杂一些的方程，形式化的定义如下：
\[
 \textbf{h} = step(W_{xh}\textbf{x} + \textbf{b}_h)
\]
\[
 y = \textbf{w}_{hy}\textbf{h} + b_y.
\]
计算过程分两步进行：计算\textbf{隐层}，这一步对输入$\textbf{x}$输出一个隐层向量$\textbf{h}$；然后计算\textbf{输出层}，这一步对输入$\textbf{h}$计算出最终的结果$y$。
两层节点都包括使用权值矩阵$W$和偏执向量$\textbf{b}$的\textbf{线性变换}操作，然后跟一个$step(\cdot)$函数，这个函数定义如下：
\[
 step(x) = \left\{ \begin{array}{ll}
  1 & \textrm{if $x > 0$} \\
  -1 & \textrm{otherwise}
  \end{array} \right.
\]
这个函数是被称为\textbf{多层感知机}的一类神经网络的一个例子。
宽泛地讲，多层感知机包含一个或多个隐层，每个隐层先进行线性变换然后再进行非线性化操作（例如这里使用的step函数），最终到达输出层，计算出不同的输出。

图8展示了之所以这种网络能够很好的处理图7所示的非线性分类问题。
简单来说，我们可以看到，第一个隐层将输入$\textbf{x}$\textit{映射}到另一个特征空间里的隐向量$\textbf{h}$，原本的不可分类问题在这个新的特征空间就变得线性可分了。具体在这个例子里，我们可以看到，$\textbf{h}$现在处在一个线性可分空间中，使得我们可以在这个空间中第一一个线性分类函数（使用$w_y$和$b_y$）就可以准确的计算出我们想要的输出值$y$。

如上所述，多层感知机是神经网络的一个具体形态。更广泛地说，神经网络可以被理解为一个链式函数（如上面所说的线性变换和step函数，当然也包括很多很多其他函数），在给定输入的情况下计算得到一些想要的输出值。神经网络的强大之处，就在于这样一个事实，即可以用链式计算的方法将一系列简单的方程连在一起，就可以表达出更加复杂的函数，而且这个复杂的函数容易进行训练，且参数可以不用那么多。
实际上，上面所介绍的一个简单的单层神经感知机是一个\textbf{通用函数逼近器}，也就是说，只要其隐层$\textbf{h}$包含足够的节点，这个模型就能用来拟合任何一个函数，拟合准确率视隐层节点数不同而不同。

我们将在5.3小节介绍训练过程，然后我们在第5.5节介绍神经网络语言模型时会给出几个例子，来说说明这种模型如何能够降低参数规模的。
\subsection{训练神经网络}
现在我们有了如公式34所示的模型，我们需要来训练其参数$W_{mh},b_{h},w_{hy},b_y$。
还记得我们上一节中介绍的梯度下降训练方法吧，我们需要定义一个损失函数$\mathcal{L}(\cdot)$，然后针对每个参数求这个损失函数的梯度，接着在降低梯度的方向上走一小步。我们使用回归问题常用的\textbf{误差的平方}作为我们的损失函数，这个损失是衡量系统输出$y$与给定的正确结果$y^*$之间的差异大小的，形式化的如下表示：
\[
 \mathcal{L}(y^*,y) = (y^* - y)^2.
\]

下一步，我们需要计算偏导数。这里，我们遇到了一个问题：这里的$step(\cdot)$函数对求导不太友好，因为其倒数形式如下：
\[
 \frac{\mathnormal{d}step(x)}{\mathnormal{d}x} = \left\{ \begin{array}{ll}
  undefined & \textrm{if $x = 0$} \\
  0 & \textrm{otherwise}
  \end{array} \right.
\]
因此，一般常用其他的非线性函数，比如双曲正切函数。如图9所示的正切函数，形状上看更像是step函数的松弛版，但是连续可导的，使其非常有利于进行梯度下降方法来训练。当然还有很多其他的选择，其中最受欢迎的是整流线性单元（ReLU）：
\[
 ReLU(x) = \left\{ \begin{array}{ll}
  x & \textrm{if $x > 0$} \\
  0 & \textrm{otherwise}
  \end{array} \right.
\]
如图9左边所示。简单来说，tanh函数有一个“饱和”问题，即当输入值$x$的绝对值很大（$x$是极大的正数或极小的负数）时偏导数会变得非常小，而整流线性单元ReLU解决了这个问题。包括本节介绍的语言模型在内，相关实验结果均指出，ReLU是tanh函数的有效替代者。

这里有个图。

那么我们就用非线性激发函数tanh来代替了我们的网络中的step函数，我们就可以像在第4.3节所做的那样，开始计算偏导数了。
首先，我们来分步计算损失函数：
\[
 \textbf{h}^{'} = W_{xh}\textbf{x} + \textbf{b}_h
\]
\[
 \textbf{h} = tanh(\textbf{h}^{'})
\]
\[
 y = \textbf{w}_{hy} \textbf{h} + b_y
\]
\[
 \mathcal{L} = (y^* - y)^2
\]
然后，再使用链式规则，我们计算每个参数的偏导数：
\[
 \frac{d\mathcal{L}}{db_y} = \frac{d\mathcal{L}}{dy} \frac{dy}{db_y}
\]
\[
 \frac{d\mathcal{L}}{d\textbf{w}_{hy}} = \frac{d\mathcal{L}}{dy} \frac{dy}{d\textbf{w}_{hy}}
\]
\[
 \frac{d\mathcal{L}}{d\textbf{b}_h} = \frac{d\mathcal{L}}{dy} \frac{dy}{d\textbf{h}} \frac{d\textbf{h}}{d\textbf{h}^{'}} \frac{d\textbf{h}^{'}}{d\textbf{b}_h}
\]
\[
 \frac{d\mathcal{L}}{dW_{xh}} = \frac{d\mathcal{L}}{dy} \frac{dy}{d\textbf{h}} \frac{d\textbf{h}}{d\textbf{h}^{'}} \frac{d\textbf{h}^{'}}{dW_{xh}}
\]

我们可以通过手算的方法，来得到模型的所有参数的精确偏导数。有兴趣的读者完全可以这么做，但即使是上面所述的这种简单模型，这样做也需要相当大的工作量和查错处理。尤其是对于更加复杂的模型，比如接下来的章节中将要介绍的模型，更是如此。

幸运的是，当我们在编程实现一个神经网络模型时，有一个非常有用的工具\textbf{自动偏微分}，可以为我们省很多事情。
为了理解自动偏微分法，把公式39中的计算过程看做是一个\textbf{计算图}数据结构会很有帮助，在图10中给出了两个示例。在这些图中，一个图结点要么表示这个网络的一个输入，或者表示一个计算过程的结果，比如乘法、加法、双曲正切，或者平方错误差。
图中的第一个计算图是一个函数的计算过程，我们在使用这个模型来做预测的时候会用到这个计算图，而第二个计算图是计算损失函数的过程，我们在训练时会用到。

自动偏微分是一个在第二个计算图上分两步进行操作的动态规划算法，具体过程分为：
\begin{itemize}
\item \textbf{前向计算}，按照计算图的拓扑结构来访问每个结点，并计算出公式39所示的实际计算结果。
\item \textbf{后向传播}，按照计算图的拓扑结构逆向访问每个结点，并计算出公式40所示的各个偏微分。
\end{itemize}
这样形式化过程的好处也很明显，根据计算图来计算的函数方程可以相对复杂些，只要这个复杂函数能够通过结合多个简单结点来得到，而这一简单结点可以用来计算函数$f(x)$和其偏微分$f'(x)$，我们就可以用自动偏微分方法来使用动态规划的计算过程来得到这个复杂函数的偏导数，而不需要徒手计算了。

因此，为了实现一个通用的神经网络模型的训练算法，需要编程实现这两个动态规划过程，包括独立的前向计算过程，和我们所需的每一种结点的后向偏微分结果。虽然这样本身不是微不足道的，现在也有一批工具包，可以用来做通用的自动偏微分，或者是针对机器学习或神经网络专门定制的自动偏微分。这种计算图的数据结构、结点、后向传播、参数最优化算法等等的开源实现，不仅可以用来高效且可靠的训练神经网络模型，也可以让实践者们开始设计自己的模型了。在下一节中，我们将介绍如何使用一个开源工具包\textbf{DyNet}来实现我们自己的模型，这个开源工具包提供的编程接口能让我们方便的实现将要介绍的端到端模型。

\subsection{一个实现样例}
图11给出了一个用\textbf{DyNet}工具来实现上面所述的神经网络的例子，我们将按行讲解这个代码。
1-2行是引用必要的包。4-5行定义了模型的必要参数：隐层向量\textbf{$h$}的大小和训练时所需进行的迭代次数。第7行初始化一个DyNet模型，我们将在其中保存所有需要学习的参数。第8-11行对参数$W_{xh}$，$b_h$，$w_{hy}$和$b_y$进行合适大小的初始化，以便能和公式39所需的要求相匹配。第12行初始化一个“训练器”，它可以按照更新规则来对模型的参数进行更新（这里我们使用了简单的随机梯度下降法，但是使用AdaGrad,Adam或其他优化方法的训练器也是存在的）。第14行为训练过程生成数据，这里我们是训练图7所示的函数。

第16-25行定义了一个函数，它的输入是\textbf{$x$}，然后生成一个计算图来完成公式39的计算过程。
首先，第17行先生成一个计算某个训练样本的计算图。
第18-21行把参数以DyNet变量的形式加到计算图中。
第22行使用Python的list来表示当前的输入，并将其作为DyNet变量加入到计算图中。
第23行计算隐层向量\textbf{$h$}，第24行计算出结果$y$，第25行返回这个计算结果。

这里有个代码图。

这里还有个计算图的图

第27-36行在训练数据上进行$\mathrm{NUM\_EPOCHS}$次迭代训练。
第28行定义一个变量来记录本轮迭代的损失总和，用于本轮迭代后的日志输出。
第29行对训练数据进行打乱顺序，和4.2小节中推荐的那样。
第30-35行进行随机梯度下降过程，顺次使用每一个训练样本进行学习。
第31行计算函数的输出值，第32行用其来计算损失函数的值。
第33行执行前向计算过程来得到损失值并将其加到本轮的损失值总和上。
第34行执行后向传播过程，第35行进行参数更新。
在这一轮迭代过程结束后，在第36行打印本轮迭代的损失值，用来观察损失值在降低，预示着我们的模型训练过程在收敛。

最后，在训练过程结束后，在第38-40行，我们输出最终的计算结果。
在实际情况中，这一步将会在另一个数据集上进行。

\subsection{神经网络语言模型}
基础部分到此告一段落，是时候把神经网络模型应用到语言模型了。
一个前馈神经网络语言模型，和我们在前一节介绍的对数线性语言模型非常相似，区别仅仅是在输出层之前多了一个或多个非线性层。

首先，我们回顾一下三元对数线性语言模型。在这个模型中，假设我们有两个特征集合，分别表示了$e_{t-1}$（其表示为$W^{(1)}$）和$e_{t-2}$（表示为$W^{(2)}$）的特征表示，该对数线性模型的公式如下：
\[
 \textbf{s} = W_{\cdot,e_{t-1}}^{(1)} + W_{\cdot,e_{t-2}}^{(2)} + \textbf{b}
\]
\[
 \textbf{p} = softmax( \textbf{s} ).
\]
这里，我们将权值矩阵的相应列向量和偏执向量加和来得到得分向量，然后用softmax操作将其转换为概率分布。

与此不同，一个单层的三元神经网络语言模型的架构如图12所示，其描述性公式如下：
\[
 \textbf{m} = concat(M_{\cdot,e_{t-2}},M_{\cdot,e_{t-1}})
\]
\[
 \textbf{h} = tanh( W_{mh}\textbf{m} + textbf{b}_h)
\]
\[
 \textbf{s} = W_{hs}\textbf{h} + \textbf{b}_s
\]
\[
 \textbf{p} = softmax(\textbf{s})
\]

在第一行，我们得到了上文$e_{i-n+1}^{i-1}$的向量表示\textbf{m}（在上述特殊情况中，我们处理三元模型即$n=3$）。
这里，$M$是一个$|V|$列$L_m$行的矩阵，其中每一列中长度为$L_m$的向量为词表中对应的词的向量表示。
这个向量叫做\textbf{词嵌入}或\textbf{词表示}，即词表中对应的某个词的实数向量表示。
用一个实数向量来表示一个词是一件很有趣的事，有趣之处在于向量中的每一维反映了这个词的不同维度的特征。
例如，向量中的某一个维度表示这个词可能是个名词，或者向量中的某个维度用来表示这个词是否是一个动物，或者另一个维度来表示这个词是否可数等。图13中给出了一个样例程序，教你如何在DyNet中实现查找一个向量的参数定义方法。

将上文中的所有词的向量首尾相接，就得到了向量$\textbf{m}$，由此可知$|\textbf{m}|=L_m * (n-1)$。
一旦我们有了这个向量$\textbf{m}$，我们就可以让其通过一个隐层来得到向量$\textbf{h}$。
这样一来，模型可以学习到反映上文信息的多个词之间的组合特征。
这样就使得模型的表达能力大大加强，能够表示比图6所示更复杂的情况。
例如，给定一个上文“cows eat”，而向量$M_{\cdot,cows}$中的某个元素表示这个词是一个“大型农场动物”（如：“cow”，“horse”，“goat”），而向量$M_{\cdot,eat}$中的某些元素对应了“eat”和相关动作（如“consume”，“chew”，“ingest”）的表示，然后，我们可以学到隐层$\textbf{h}$中的某个节点，当上文表示“农场动物吃的东西”时这个节点就会被激发。

下一步，我们来计算每个词的得分向量：$\textbf{s} \in \mathbb{R}^{|V|}$。
这个得分向量可以通过对隐向量$\textbf{h}$做变换而得到，这个过程会用到权值矩阵$W_{hs} \in \mathbb{R}^{|V| \times |h|}$和偏执向量$\textbf{b}_s \in \mathbb{R}^{|V|}$。
最后，把计算得到的得分向量通过softmax函数，从而得到了概率分布$\textbf{p}$，就像我们在对数线性语言模型中做的一样。
至于训练过程，如果我们知道词$e_t$，我们可以如下计算损失函数，和对数线性模型的做法类似：
\[
 \mathcal{L} = - \log (p_{e_t}).
\]
DyNet有一个很方便的函数，可以在给定得分向量$\textbf{s}$时计算出负对数似然损失值：

这里有个代码。

当我们将神经网络模型与第3节中的$n$-gram语言模型相比较时，就不难发现神经网络这种建模方式的好处了：
\begin{enumerate}
\item[] \textbf{更好的泛化上文}：$n$-gram语言模型将每个词作为独立的个体对待。通过使用输入表示向量$M$，就可以让相似的词聚在一起，并让这些相似词在预测下一个词时具有类似的预测性质。为了达到同样的目的，$n$-gram模型需要额外的训练出一个词类来，而且能有效的使用这个词类信息并不是一件简单的事情。
\item[] \textbf{上文中的词的更泛化的组合}：在一个$n$-gram语言模型中，我们需要以参数的形式记录\{cow,horse,goat\}$\times$\{consume,chew,ingest\}这两组词的所有组合情况，来表达上文“农场动物吃的东西”这个意思。这样一来，参数规模和类中词的个数成平方关系，而在训练数据非常有限的情况下，学习这么大规模的参数是非常困难的。神经网络模型通过学习隐层表示，来用有限的参数来表示这种大规模组合关系，从而解决了这个参数膨胀问题。
\item[] \textbf{能够跳过前驱词的能力}：$n$-gram语言模型常常从较长的上文（如，“两个上文词$e_{t-2}^{t-1}$”）顺序地回退到较短的上文（如，“前一个词$e_{t-1}$”），但这个回退过程不允许“跳过”某个词，比如只考虑和“两个词之前的词$e_{t-2}$”的相关性。对数线性模型和神经网络模型则能够很自然的解决这个远距离依赖问题。
\end{enumerate}

\subsection{扩展阅读}
除了上面介绍的方法之外，神经网络语言模型还有很多其他的扩展问题值得探讨。
\begin{enumerate}
\item[] \textbf{Softmax估计}：训练对数线性语言模型或神经网络语言模型时会有一个问题，在学习每一个训练样本时，都需要计算一个很大的得分向量$\textbf{s}$，然后通过softmax函数来得到相应的概率分布。当词表规模$|V|$变大时，这一过程会非常耗时。因此，有了很多种降低训练时间的方法。其中的一个例子，就是先在词表中采样出一个子集$V' \in V$使得$|V'| << |V|$，然后在这个子集上进行得分计算和损失估计等操作。这种方法的例子，还有让这个子集中的其他词的模型得分低于真实词$e_t$的模型得分，还有一些从概率角度出发的方法，比如\textbf{importance sampling}或\textbf{noise-contrastive estimation}。有趣的是，对于其他的目标函数如线性回归、特殊的softmax方法\textbf{spherical softmax}，可以通过使用不同的目标函数，来让计算目标函数的过程并不与词表大小成正相关。
\item[] \textbf{其他Softmax架构}：提高训练速度的另一种有趣的技巧，就是创造一个具有很好结构的softmax方法，使得计算损失函数的过程可以很快。一种常见的方法就是基于类的softmax，这种方法给每个词$e_t$赋予一个类别$c_t$，然后把计算过程分为两步：给定上文时计算词类$c_t$的概率，然后在给定词类和当前上文的情况下计算词$e_t$的概率分布$P(e_t|c_t,e_{t-n+1}^{t-1})P(c_t|e_{t-n+1}^{t-1})$。这种方法的一大优势是，我们只需要计算$|C|$个类中正确的词类$c_t$的模型得分，然后再从词类$c_t$中的词条中计算目标词$e_t$的模型得分，其大小仅为$|V_{c_t}|$。因此，我们的计算复杂度变成了$O(|C| + |V_{c_t}|)$，而不是$O(|V|)$。层次softmax方法更进一步地，通过一个二叉树来预测一个词，使得计算复杂度降到了$O(\log_2 |V|)$。
\item[] \textbf{其他词表示学习模型}：如5.5小节中讲的那样，词的嵌入表示$M$是训练语言模型过程的一个副产品。词的嵌入式表示有一个非常好的地方，可以通过语言模型在未标注文本上进行训练来获得，而得到的词表示可以捕获词语的语义或句法信息，因此这个词表示信息可以有效的增强后续的其他任务的效果，比如词性标注或句法分析等，而这些任务往往没有太多的标注语料。正因为这么有用，现在已经有很多不同的方法来学习到不同的词嵌入表示，从早期基于分布相似性和降维方法，到最近的类似语言模型的基于预测模型的方法，现在通常的看法认为，两类方法相比之下，预测模型会更有效和更灵活。最著名的方法是软件$\textbf{word2vec}$中实现了的continuous-bag-of-words模型和skip-gram模型，两个方法定义了一个简单的目标函数，即基于紧邻的上下文词来预测当前词，或者通过当前词来预测其紧邻上下文中的词。\textbf{word2vec}还使用了采样机制和多线程等加速方法，使其能够快速的在大量的文本上进行快速训练，这一点也许是这个工具包如此流行的主要原因了吧。值得一提的是，这些方法所使用的模型本身都不是语言模型，因为他们都不计算一个句子的概率值$P(E)$，但很多参数估计学习的方法和语言模型中的方法是类似的。
\end{enumerate}

%\subsection{练习}

\section{循环神经网络语言模型}
在上一章节介绍的神经网络语言模型，比起$n$-gram语言模型来说，具有更加强大的泛化能力。在本章节，我们将讨论基于循环神经网络（RNN）的语言模型，这个模型还能够捕获语言中远距离的依赖关系。

\subsection{语言中的远距离依赖}
这里有个图。

在讨论循环神经网络之前，考虑一下前文讲述的语言模型的缺点，当对语言中的所有现象进行建模时，一个基于有限上文的模型往往捉襟见肘。

在图14中给出了一个远距离语法依赖的例子。在这个例子中，句首的“he”或“her”对句尾的“himself”或“herself”有很强的限制关系。类似的，根据句子的主语不同，句中的动词也会变化。不管两个词之间有多少个词，这种依赖关系是一直存在的，使用一个有限上文$e_{i-n+1}^{i-1}$的模型，如前文中讨论的那些模型，是无法捕获这种远距离的依赖关系的。这种依存关系在英语中很常见，在俄语等其他语言中更是如此，因为俄语中的词有多种变体形式，而这些变体形式必须和句子中其他词的性、数、格相对应。

另一个能说明远距离依存关系存在的情况是\textbf{指代关系}。简单来说，指代关系指的就是与“什么对什么做了什么”这类问题相关的常识。举个例子来说，“I ate salad with a fork”中可以很容易的理解“a fork”是一个工具，而“I ate salad with my friend”也是讲得通的，“my friend”可以理解为同伴。而另一方面，“I ate salad with a backpack”就不怎么讲得通了，因为“backpack”既不是吃饭用的工具，也不是一个同伴。这种违反指代规则的情况会导致无意义的句子，因为主语、动词、宾语可以相隔很远，所以这种指代相关规则可以跨越很长的范围。

这里有个图。

最后，也有一些句子或文档的\textbf{主题}或\textbf{register}相关的依赖关系。
例如，如果一个文档讨论的是技术主题的内容，突然出现一些体育方面的内容，就会显得很奇怪，即出现了违反主题一致性规则的情况。
如果一个学术论文突然使用一些不正式的口语化的语言，也会显得极不自然，这就是缺乏register一致性的情况。

这些例子，以及其他的例子，都说明了在制作可用的应用软件时，我们之所以需要对远距离依存关系进行建模的原因。

\subsection{循环神经网络}
\textbf{循环神经网络}（RNN）是一类神经网络的统称，他们使得对远距离依赖的建模成为了可能。
其思路是很简单的，通过加一条循环边，在计算当前时刻的隐层状态$\textbf{h}$时将前一时刻的隐层状态$\textbf{h}_{t-1}$也考虑在内，公式化的描述为：
\[
 \textbf{h}_t = \left\{ \begin{array}{ll}
  tanh(W_{xh}\textbf{x}_t + W_{hh}\textbf{h}_{t-1}+\textbf{b}_h) & t \geq 0 \\
  \textbf{0} & \textrm{otherwise}
  \end{array} \right.
\]
我们可以看到，在$t \geq 1$时刻，跟标准神经网络中隐层的计算过程的唯一不同之处，就是通过加入$W_{hh}\textbf{h}_{t-1}$项来将$t-1$时刻的隐层状态和$t$时刻的隐层状态连接起来了。因此这个函数变成了一个递归函数，即计算过程依赖于前一时刻的隐层状态$\textbf{h}_{t-1}$。在图15中，我们可以看到循环神经网络在当前时刻的计算图拓扑结构。

在对RNN做这样的可视化展现时，通常需要在时间序列上对神经网络进行“循环展开”，就像图15中所示那样，这样能让我们更清晰的看出在多个时间点上的信息传递过程。通过对网络的循环展开，我们可以看到，我们将循环网络的计算过程又规约为了标准的计算图上的计算过程，就像在计算前馈神经网络时所作的那样，在这个展开的计算图拓扑结构上我们可以进行前向计算和错误的后向传递，让我们可以学习模型的参数。这样也能很清楚的知道，循环神经网络必须从一个初始隐层状态$\textbf{h}_0$开始计算。这个初始状态一般会被设为一个全零向量，或者被作为参数$\textbf{h}_{init}$通过学习得到，或者通过其他信息来进行初始化（更多相关讨论见第7节）。

最终，为了简便起见，通常会把循环神经网络的一个完整计算过程简称为“RNN”计算模块，如图15所示。在这个例子中，对应于RNN函数的方框都是灰色的，意思是他们内部包含参数$W_{xh}$，$W_{hh}$和$b_h$。我们会在后面的介绍中沿用这种约定俗称来表示带参数的函数。

由于有了能让信息在不同时间节点间的传递能力，RNN模型使得对远距离依存关系的建模成为了可能。例如，如果$\textbf{h}_{t-1}$中的某些节点表示了“这个句子的主语是个男性”这样一条信息，可以将这个信息传递到$\textbf{h}_t$，进而传递给$\textbf{h}_{t+1}$，从而能一直传递到句子结束为止。这种在很长的时间序列上传递信息的能力，就是循环神经网络的过人之处，这也使得这个模型能够解决6.1节中讲述的远距离依存关系的建模问题。

一旦我们了解了RNN的基础知识，将其应用到语言模型建模过程就比较直观了。我们只是在公式42所示的前馈神经网络语言模型中加入一条循环边而得到如下的形式化表示：
\[
 \begin{array}{l}
 \textbf{m}_t = M_{\cdot,e_{t-1}} \\
 \textbf{h}_t = \left\{ \begin{array}{ll}
  tanh(W_{xh}\textbf{x}_t + W_{hh}\textbf{h}_{t-1}+\textbf{b}_h) & t \geq 0 \\
  \textbf{0} & \textrm{otherwise}
  \end{array} \right. \\
 \textbf{p}_t = softmax(W_{hs}\textbf{h}_t + b_s).
 \end{array}
\]
在这里值得一提的是，跟前馈神经网络语言模型不同的是，我们这里只输入前一个词，而不是前驱的两个词。这是因为，我们期望词条$e_{t-2}$和所有其前面的词的信息已经包含在$\textbf{h}_{t-1}$里了，这样也就不需要直接输入这些前驱词的信息了。

还有，为了表述方便，通常会用$RNN(\cdot)$函数来简化$\textbf{h}_t$的计算公式，如图15中RNN的简化画法相对应地，我们可以将上述公式简化表述为：
\[
 \begin{array}{l}
 \textbf{m}_t = M_{\cdot,e_{t-1}} \\
 \textbf{h}_t = RNN(\textbf{m}_t, \textbf{h}_{t-1}) \\
 \textbf{p}_t = softmax(W_{hs}\textbf{h}_t + b_s).
 \end{array}
\]

\subsection{梯度消失和长短记忆模型}
然而，虽然前一节中介绍的RNN模型在概念上是很简单的，但是它也有自身的问题：一个是\textbf{梯度消失}问题，另一个姊妹问题是\textbf{梯度爆炸}问题。

在图16中，给出了梯度消失问题的一个概念化示例。在这个例子中，我们有一个循环神经网络模型，比如是一个用来对文档进行分类的模型，亦或是在给定一段序列文本后进行某种预测行为的模型，会在经过几步RNN操作后做出一个预测行为。
完成这个预测后，就可以计算出损失函数值，并期望这个损失值能够在后向传播过程中传播到神经网络的每个时间节点处。
然而，在每一个时间节点，当我们执行后向传播操作时，梯度值会越来越小，当传播到句子的开始部分时，我们的梯度值会变得极小，以至于对参数更新而言，这个极小的梯度几乎起不到有效的更新作用。
造成这个问题的主要原因，是因为，除非$\frac{d\textbf{h}_{t}}{d\textbf{h}_{t-1}}$的值等于1，否则它就会趋向于让梯度值$\frac{d\mathcal{L}}{d\textbf{h}_t}$变得极小或极度放大，尤其是这种减少或放大行为多次进行后，会对损失函数的梯度造成指数级影响。

一种解决这个问题的方法，比如说解决梯度消失问题的方法，是通过针对性的设计一个神经网络架构来保证循环函数的梯度值为1。
一种被称为\textbf{长短距离记忆}模型（LSTM）的神经网络架构，就是为这个目的而设计的一个神经网络架构，在多种序列处理问题建模过程中，该模型获得了相当大的成功和极大的关注度。
LSTM模型背后最基本的思想，就是在大多数神经网络常用的标准隐层状态$\textbf{h}$的基础上，它还加入了一个\textbf{记忆单元层}$\textbf{c}$，而它的梯度$\frac{d\textbf{c}_t}{d\textbf{c}_{t-1}}$等于1。
正因为这个梯度值为1，存储在记忆单元的信息不会遭遇梯度消失问题，因此，相比于标准的循环神经网络，LSTM模型能够更有效的捕获远距离依存关系。

LSTM模型是如何做到这一点的呢？为了理解其中的奥秘，我们通过图17形象而具体地看一下LSTM的架构是怎么样的，下面是其形式化的公式表达：
\[
 \begin{array}{l}
 \textbf{u}_t = tanh(W_{xu}\textbf{x}_t + W_{hu}h_{t-1} + \textbf{b}_u) \\
 \textbf{i}_t = \sigma (W_{xi}\textbf{x}_t + W_{hi}h_{t-1} + \textbf{b}_i) \\
 \textbf{o}_t = \sigma (W_{xo}\textbf{x}_t + W_{ho}h_{t-1} + \textbf{b}_o) \\
 \textbf{c}_t = \textbf{i}_t \odot \textbf{u}_t + \textbf{c}_{t-1} \\
 \textbf{h}_t = \textbf{o}_t \odot tanh(\textbf{c}_t).
 \end{array}
\]
我们一个公式一个公式地看：
公式47是更新函数，基本上和RNN中的更新函数44类似；接收输入向量和隐层状态，执行一次线性变换后经过tanh函数进行非线性化。

公式48和公式49，分别是LSTM模型中的\textbf{输入门}和\textbf{输出门}。“门”所要进行的操作，根据其名字就能看出，就是让信息要么通过这个门，要么被拒之门外而阻断其传播。这两个门都进行一次线性变换然后经过\textbf{sigmoid函数}，这个函数也被称为\textbf{逻辑斯蒂函数}：
\[
 \sigma (x) = \frac{1}{1 + exp(-x)}
\]
这个函数将输入值$x$压缩到0至1的范围内，当$x$的值趋向于负无穷时$\sigma(x)$的值会趋向于0，当$x$的值趋向于正无穷时$\sigma(x)$的值会趋向于1。Sigmoid函数的输出，会用来和另一个函数的输出结果进行分量型乘法操作：
\[
 \begin{array}{l}
 \textbf{z} = \textbf{x} \odot \textbf{y} \\
 z_i = x_i * y_i
 \end{array}
\]
“门”效应的结果：如果向量的某个维度的sigmoid结果接近1，这个点上对输入没有什么影响（即这个门处于“开启”状态），如果其sigmoid值接近0，则这个输入会被阻截，导致乘积结果为0（即这个门处于“关闭”状态）。

公式50是LSTM模型中最重要的公式，因为它是“让梯度$\frac{d\textbf{c}_t}{d\textbf{c}_{t-1}}$必须等于1”这个直观目的的实现细节，也正是我们解决梯度消失问题的关键步骤。这个公式让$\textbf{c}_t$的值等于输入门$\textbf{i}_t$和更新门$\textbf{u}_t$的外积加上前一个时间节点的$\textbf{c}_{t-1}$。因为我们是直接将$\textbf{c}_{t-1}$作为了计算$\textbf{c}_t$时的一个加和项，如果我们只看公式50中的这个加和部分，不难看出这里的梯度$\frac{\frac{d\textbf{c}_t}{d\textbf{c}_{t-1}}}{}$确实为1。

最后，公式51用来计算LSTM的下一个隐层状态。计算过程如下，先将记忆单元的值通过tanh函数变换到-1到1范围内，然后将结果和输出门的结果$\textbf{o}_t$进行外积。这个隐层状态值会在后续的计算过程中用到，比如用于计算语言模型的概率分布：
\[
 \textbf{p}_t = softmax(W_{hs}\textbf{h}_t + b_s)
\]

\subsection{RNN的其他变种}
由于在多个应用中都证明了循环神经网络的重要性，于是有了这种神经网络的多种变体存在。对标准的LSTM模型的一个常见改进模型是引入一个\textbf{忘却门}，实际上，这个改进版的模型已经常见到，当人们说“LSTM”模型时都指的是这个改进版本。带忘却门的LSTM的相关公式如下：
\[
 \begin{array}{l}
 \textbf{u}_t = tanh(W_{xu}\textbf{x}_t + W_{hu}h_{t-1} + \textbf{b}_u) \\
 \textbf{i}_t = \sigma (W_{xi}\textbf{x}_t + W_{hi}h_{t-1} + \textbf{b}_i) \\
 \textbf{f}_t = \sigma (W_{xf}\textbf{x}_t + W_{hf}h_{t-1} + \textbf{b}_f) \\
 \textbf{o}_t = \sigma (W_{xo}\textbf{x}_t + W_{ho}h_{t-1} + \textbf{b}_o) \\
 \textbf{c}_t = \textbf{i}_t \odot \textbf{u}_t + \textbf{f}_t \odot \textbf{c}_{t-1} \\
 \textbf{h}_t = \textbf{o}_t \odot tanh(\textbf{c}_t).
 \end{array}
\]
跟标准的LSTM模型相比，这里有两处变化。首先来看公式54，我们引入了忘却门。其次，在公式55中，在将前一时刻的$c_{t-1}$传给当前时刻的$c_t$之前用忘却门的结果去与之做个外积。忘却门在一些情况下非常有用，当需要时忘却门能让记忆单元轻易地清空记忆内容，比如，我们不妨假设这个模型记住了一个特定的词条和另一个词条有很强的相关性，比如上个例子中的“he”和“himself”，或者“she”和“herself”。在这个例子中，我们期望模型能够在预测“himself”之前都一直记忆“he”的信息，而预测结束后就忘却这个信息，就好像那两个词再也不相关了一样。忘却门就有这种优势，它能让这种精细的信息流控制成为可能，当然它也带着一个隐患随行，如果$\textbf{f}_t$在每个时刻都被置为0，这个模型会忘却所有信息进而失去捕获远距离依存关系的能力。因此，在开始训练神经网络模型之前，通常都会讲忘却门的偏执参数$\textbf{b}_f$初始化为一个较大的值（比如1），这样能保证神经网络在开始训练时是不会用到忘却门的，随着训练过程的进行，慢慢地开始忘却一些内容。

虽然LSTM模型为梯度消失问题提供了一个有效的解决途径，但这个模型还是太复杂了（很多读者肯定已经感受到了）。另一个简单的RNN变体是基本已被证明有效的\textbf{gated recurrent unit}(GRU)，其形式化公式如下：
\[
 \begin{array}{l}
 \textbf{r}_t = \sigma (W_{xr}\textbf{x}_t + W_{hr}h_{t-1} + \textbf{b}_r) \\
 \textbf{z}_t = \sigma (W_{xz}\textbf{x}_t + W_{hz}h_{t-1} + \textbf{b}_z) \\
 \tilde{\textbf{h}}_t = tanh(W_{xh}\textbf{x}_t + W_{hh}(\textbf{r}_t \odot \textbf{h}_{t-1}) + \textbf{b}_h) \\
 \textbf{h}_t = (1 - \textbf{z}_t)\textbf{h}_{t-1} + \textbf{z}_t \tilde{\textbf{h}}_t.
 \end{array}
\]
GRU模型最独特的一点就是公式59，即对已更新的隐层状态$\tilde{\textbf{h}}_t$和前一时刻的状态$\tilde{\textbf{h}}_{t-1}$进行插值。
这个插值过程受到\textbf{更新门$z_t$}的控制，当更新门的值趋近1时，GRU会使用最新的隐层状态，而当更新门的值趋近0时，它就会使用前一时刻的隐层状态。隐层状态候选的计算过程如公式58所示，和标准的RNN更新过程类似，但它包含了一步操作，即用公式56计算出的\textbf{重置门$r_t$}来对前一时刻的隐层状态输入进行外积。跟LSTM相比，GRU包含的参数稍微少一些，并且没有单独的“记忆单元”的概念。因此GRU模型常常被用来减少内存和计算方面的开销。

这里有个图。

我们可以对RNN模型、LSTM模型、GRU模型或者任何神经网络层做一个简单但是非常有效的改进：将多个层一个个堆叠起来（\textbf{多层RNN模型}如图18）。例如，在一个3层RNN模型中，在$t$时刻的计算过程会如下进行：
\[
 \begin{array}{l}
 \textbf{h}_{1,t} = RNN_1 (\textbf{x}_t,\textbf{h}_{1,t-1}) \\
 \textbf{h}_{2,t} = RNN_2 (\textbf{h}_{1,t},\textbf{h}_{2,t-1}) \\
 \textbf{h}_{3,t} = RNN_3 (\textbf{h}_{2,t},\textbf{h}_{3,t-1}),
 \end{array}
\]
这里$\textbf{h}_{n,t}$是第$n$层在第$t$时刻的隐层状态，而$RNN(\cdot)$是对公式44中的RNN函数计算过程的简称。
类似的，我们可以将这个函数替换为$LSTM(\cdot)$，$GRU(\cdot)$，或者任何其他的循环网络函数。
之所以将多层神经网络堆叠起来会很有用，是因为在第5节介绍的标准神经网络中的非线性变换被证实是非常有用的：它们可以从当前的词或句子中逐层抽取出更加抽象的特征信息。例如，据文献【】研究发现，在一个双层的LSTM中，第一层会倾向于学习诸如词性的词的细粒度特征，而第二层会学到诸如句子的音色时态等更抽象的特征。

虽然堆叠的RNN模型具有很多潜在的好处，但也有一些不足之处，比如会遭遇纵向的梯度消失问题，就跟标准的RNN模型在横向上遇到的梯度消失问题一样。那也就是说，梯度会从接近输出层的$RNN_3$反向传播到接近输入层的$RNN_1$，在这个传播过程中梯度可能会消失，导致整个网络的前几层训练不到位。和LSTM解决梯度消失问题的方法类似，解决这个问题也有一个简单的策略，叫\textbf{残差网络（residual networks）}(如图18)。这些网络架构背后的思想其实比较简单，就是将前一层的输出加到当前层的输出上，就像下面的公式所述：
\[
 \begin{array}{l}
 \textbf{h}_{1,t} = RNN_1 (\textbf{x}_t,\textbf{h}_{1,t-1}) + \textbf{x}_t \\
 \textbf{h}_{2,t} = RNN_2 (\textbf{h}_{1,t},\textbf{h}_{2,t-1}) + \textbf{h}_{1,t} \\
 \textbf{h}_{3,t} = RNN_3 (\textbf{h}_{2,t},\textbf{h}_{3,t-1}) + \textbf{h}_{2,t}.
 \end{array}
\]
这样一来，就像LSTM一样，就不存在梯度传播过程中因为经过$RNN(\cdot)$函数而消失的问题，即使是很深的网络架构也会有效的学习到每层的参数。

\subsection{实时，整块，小块训练}
善于观察的读者可能已经注意到了，前一节中循序渐进的介绍了一个比一个复杂的模型，我们从一个简单的线性模型开始，加入了一个隐层，又加入了循环性，加入了LSTM，然后加入了多层的LSTM。虽然这些更有表达性的模型有能力达到更高的精度，但这也是有代价的：大幅增长的参数空间（导致极有可能过拟合）和更加复杂的运算过程（导致需要更多的计算消耗）。本节将介绍一种能提升训练这些复杂模型的稳定性以及计算效率的有效方法，\textbf{minibatching}。

到目前为止，我们已经用了4.2节中介绍过的随机梯度下降学参算法，其更新过程可以描述为下面的迭代过程。每次根据一个样本来进行参数学习更新的这类学习方法，我们称之为\textbf{online learning}。

这里有个伪代码图。

相反地，我们也可以想到\textbf{batch learning}算法，这种学习方法会将这个训练数据作为一个单元，在这个整体单元上计算梯度值，每次遍历完整个训练数据后才进行参数更新。

这两种更新策略需要进行折中考虑。
\begin{itemize}
\item 因为不需要遍历完整的数据就可以进行参数更新，所以在线学习算法通常能更快地找到相对更好的解。
\item 然而，由于不会受最近看到的训练样本的过度影响，在训练快要结束的阶段，批量学习算法会更稳定。
\item 批量学习算法也更容易陷入局部最优；在线学习中的随机机制，可以让其能够跳出局部最优点从而找到更好的全局解。
\end{itemize}

这里有个算法图和示意图。

小块训练方法是这两种策略的一个很好的折中。基本上，小块训练和在线学习过程相似，只不过在每次学习参数时不再是只考虑一个样本，而是考虑$n$个样本来计算这些样本整体的梯度。在$n=1$这个特殊情况下，这种学习方法就跟在线学习是一样的了，而当$n$等于整个训练数据的大小时，这种学习方法又等同于批量学习方法。在训练语言模型时，每次参数更新前需要学习$n=1$到$n=128$个句子来作为数据块。当我们增加训练样本数时，每次参数更新过程会更有信息量且更稳定，但每轮更新参数所消耗的时间也会增加，所以，通常我们会从这两个方面的平衡的角度出发去选择$n$的大小。

小块训练方法的另一个主要益处是：通过使用一些小技巧，可以让同时处理$n$个训练样本所消耗的时间比单独处理$n$个样本所消耗的时间要少的多。具体地，当我们同时处理多个训练样本且将计算过程中相同的操作分组同时处理时，我们可以得到更大的计算效率上的增益，这是因为有这样一个事实的存在，现在的硬件设备（特别是GPU，当然也包括CPU）拥有非常有效率的向量处理指令，可以利用适当结构化的输入数据来提升计算效率。如图19所示，在神经网络中的这种操作重组的例子包括：把多个训练样本上的矩阵和向量相乘的过程组合为矩阵和矩阵相乘的过程，或者，在多个向量上同时进行诸如tanh等原子操作，而不是一个向量一个向量地单独计算。幸运的是，在我们所使用的DyNet包中，做到这一点相对比较容易，因为很多这种机械的原子操作都会被自动的归并处理。我们将在下文中给出一个示例，来说明当我们实现一个RNN语言模型时如何来做这些改进优化。

小块RNN语言模型的基本思想是这样的，我们一次性处理多个句子，而不是每次处理一个句子。因此，我们需要同时查找多个词的词向量，而不是一个词一个词的查其向量表示。然后我们向正常情况下那样将这些成块儿的词向量输入到RNN和softmax函数中，得到两个独立的概率分布，分别表示第一句和第二句中的词的概率分布。然后我们计算每个词的损失函数值。我们将这些损失值加起来作为整个句子的损失值。

然而，还有一个具体问题，那就是，如图中所示的那样，我们所造出来的小块数据里很可能存在不同长度的句子。在这种情况下，为了确保能够正常处理不同长度的句子，通常需要进行\textbf{句子补全}和\textbf{masking}操作。句子补全操作很简单，就是在小块中长度较短的句子后面加上一个或多个“句末符”，从而使其长度和这块数据中最长的句子相等，这样就保证了同一块数据中所有句子都是等长的了。Masking是这样进行的，对前面所述的人为添加的“句末符”，在计算损失函数值时要乘以0，这样就确保了短句中的多个句末符不会被重复计算损失值。

通过这样两个操作，我们就可以同时处理不等长的多个句子了，但任然有一个问题：如果我们的小块数据中的句长差异很大，那我们就需要人为添加很多的句末符来填充位置，而过多的句末符填充，就会浪费过多的计算资源。为了解决这个问题，通常会对训练语料中的句子按句子长度进行排序，然后在制作小块，来确保每个小块中的句子都差不多是等长的。

\subsection{扩展阅读}
因为RNN模型在包括自然语言和其他数据的多个处理应用中都很受欢迎，因此对其扩展的兴趣一直比较大。下面列几个人们正在着手研究的课题：
\begin{enumerate}
\item[] \textbf{循环神经网络能学到什么？}RNN是用来处理语言的十分强大的工具，因此很多研究人员都对其内部到底在发生什么很感兴趣。文献【】展示了对LSTM网络内部状态的几种可视化方法，发现有些节点会负责跟踪句子的长度，或者是是否开启了括号，或者其他显著的句子特征。文献【】给出了几种方法，通过在基于RNN的模型在其神经网络上向后传递的信息，来分析和可视化对模型最终判定起决定性贡献的输入部分。
\item[] \textbf{其他的RNN架构：}还有很多其他的循环神经网络架构。文献【】进行了一项很有意思的研究，他们通过对LSTM的不同部分进行去除的研究，来查找针对具体任务而言最好的神经网络架构。文献【】在这个基础上更进一步，具体地训练一个模型，来查找这种最优的神经网络架构。
\end{enumerate}

%\subsection{练习}


\section{神经编码-解码模型}
从第3节到第6节，我们把关注点放在了用于计算一个序列$E$的概率分布$P(E)$的语言模型问题上。在本节中，我们回到统计机器翻译问题上来，即在给定输入$F$的情况下计算输出$E$的概率分布$P(E | F)$的建模问题。

\subsection{编码-解码模型}
我们将介绍的第一个模型是\textbf{编码-解码}模型。这个模型背后的基本思想相对来说比较简单：我们有一个RNN语言模型，但在开始计算$E$的概率分布之前，我们先用另一个RNN模型在源语言句子$F$上计算出这个模型的每一个内部隐层状态。“编码-解码”这个名字是从这样一个概念而得来的，第一个神经网络将源语言句子$F$的信息“编码”成一个实数值向量（隐层状态向量），然后第二个神经网络把这个信息“解码”为目标语言句子$E$。

这里有个图。

如果我们用$RNN^{(f)}(\cdot)$来表示这个编码器，用$RNN^{(e)}(\cdot)$来表示这个解码器，然后，我们还有个softmax函数来接收$RNN^{(e)}$在时刻$t$的隐层状态作为输入来将其转换成概率分布，那么，我们的模型可以描述为如下几个公式：
\[
 \begin{array}{l}
   \textbf{m}_t^{(f)} = M_{\cdot,f_t}^{(f)} \\
   \textbf{h}_t^{(f)} = \left\{ \begin{array}{ll}
      RNN^{(f)}(\textbf{m}_t^{(f)},\textbf{h}_{t-1}^{(f)}) & t \geq 1 \\
      \textbf{0} & \textrm{otherwise}
      \end{array} \right. \\
   \textbf{m}_t^{(e)} = M_{\cdot,e_{t-1}}^{(e)} \\
   \textbf{h}_t^{(e)} = \left\{ \begin{array}{ll}
      RNN^{(e)}(\textbf{m}_t^{(e)},\textbf{h}_{t-1}^{(e)}) & t \geq 1 \\
      \textbf{h}_{|F|}^{(f)} & \textrm{otherwise}
      \end{array} \right. \\
   \textbf{p}_t^{(e)} = softmax(W_{hs}\textbf{h}_t^{(e)} + b_s)
 \end{array}
\]
在前两行，我们查到词向量$\textbf{m}_t^{(f)}$然后计算编码器在源语言句子$F$上第$t$时刻的隐层状态$\textbf{h}_t^{(f)}$。
我们从一个零向量$\textbf{h}_0^{(f)}=\textbf{0}$作为初始状态来开始计算，而当计算到$\textbf{h}_{|F|}^{(f)}$时，编码器已经把源语言句子中的所有词都遍历完了。因此，这个隐层状态在理论上是对源语言句子中的信息的编码结果了。

在解码阶段，我们在每个时刻预测词条$e_t$的概率分布。首先，类似地，我们先查找词向量$\textbf{m}_t^{(e)}$，但是我们这次试用前一个词$e_{t-1}$，因为我们必须把$e_t$的概率分布建立在前一个词$e_{t-1}$的条件上，而不是以自己作为条件。然后，我们用解码器来计算其隐层状态$\textbf{h}_t^{(e)}$。这一计算过程和编码阶段的计算过程很类似，唯一不同在于，为了让解码器在给定$F$的条件下进行计算，我们把解码器的初始状态$\textbf{h}_0^{(e)}$设为编码器的最终状态$\textbf{h}_{|F|}^{(f)}$。最后，我们用softmax函数在隐层状态$\textbf{h}_t^{(e)}$上计算概率分布$\textbf{p}_t^{(e)}$。

虽然这个模型相当简单（只有5行公式而已），但给了我们提供了对$P(E|F)$建模的一个直观且强大的方法。
事实上，【】指出，这样一个简单的模型也能进行翻译操作，其翻译效果甚至和那些针对机器翻译任务而经过特别重度优化过的系统一样好。

\subsection{生成结果}
到目前为止，我们只提到了如何创建一个概率模型$P(E|F)$，但没有讲述如何用这个概率模型来生成翻译结果，而这也是我们下一小节将讨论的内容。简单来说，我们可以使用如下几种方法来生成翻译结果：
\begin{enumerate}
\item[] \textbf{随机采样：}从概率分布$P(E|F)$上随机选择一个翻译输出$E$。通常把这个工程表示为$\hat{E} \sim P(E|F)$。
\item[] \textbf{1-best搜索：}搜索一个$E$使得$P(E|F)$最大化，表述为$\hat{E} = \arg\max \limits_{E} P(E|F)$。
\item[] \textbf{n-best搜索：}搜索概率值$P(E|F)$最高的$n$个翻译候选结果。
\end{enumerate}
选择哪种方法来生成翻译结果，我们需要依据具体的应用需求而定，因此我们将在介绍具体算法的同时会顺带着介绍一些使用样例。

\subsubsection{随机采样}
当我们需要对同一个输入得到多个不同的输出时，\textbf{随机采样}是个很有用的方法。一个这种方法很有用的使用情景，是用端到端模型来处理对话系统的情况，因为我们不希望对话系统对用户的同一个输入内容总是回复相同的回答，即不希望系统的回答不那么单调。幸运的是，在上述的编码-解码模型中，从概率分布$P(E|F)$采样生成候选是非常简单的事，有一种叫做\textbf{祖采样}的方法就能帮我们做到这一点。祖采样过程是这样进行的，在每个时间点采样多个样本，逐步扩展上文内容，在第$t$时刻，我们从概率分布$P(e_t | \hat{e}_1^{t-1})$中采样出一个词。在编码-解码模型中，这意味着我们只需要根据前面采样出来的输入来计算概率$\textbf{p}_t$，就引出了如【】所示的简单的生成算法。

值得一提的是，有时候我们想要知道我们采样出来的句子的概率值。例如，对于模型生成的一个句子$\hat{E}$，我们想要知道模型对于它所生成的这句话的置信度。在采样过程中，我们可以通过循序渐进的通过将每一个采样出来的词的概率相乘来计算句子概率$P(\hat{E}|F) = \prod_{t}^{|\hat{E}|}P(\hat{e}_t|F,\hat{E}_1^{t-1})$。然而，我们还记得在3.3节讨论的概率与对数概率的情况吧，直接使用概率值来计算常常会在计算机系统中产生浮点数溢出的问题。因此，在计算整句话的概率时，为了防止这种浮点数溢出问题的出现，通常都会将概率乘积换成对数概率之和的方法来计算。

算法图3

\subsubsection{贪心1-best搜索}
下面，我们来考虑生成1-best最佳结果的问题。这种生成方式，在机器翻译这种我们只想让模型输出它认为最佳的译文的应用中非常有用。
最简单的做法就是\textbf{贪心搜索}，即，在每个时刻计算出概率分布$\textbf{p}_t$，从这个分布中选择概率最大的那个词，然后将这个词作为结果序列中的下一个词。换句话说，这个生成算法和算法3完全一样，区别只是在第9行上，在这里我们选择最大概率词$\hat{e}_t = \arg \max \limits_{i} p_{t,i}^{(e)}$，而不是根据概率分布$\textbf{p}_t^{(e)}$来随机采样出$\hat{e}_t$。

有趣的是，祖采样方法能按照概率分布$P(E|F)$精确的采样出输出结果，贪心搜索却不能保证能搜索到概率最大的译文。
在图22中，可以看到一个能说明这是事实的一个样例，这个示例是一个词表为\{a，b，</s> \}的搜索图。作为练习，我鼓励读者去找一下给定概率分布$P(E|F)$时的真实的1-best（或n-best）句子出来，再计算一下贪心搜索得到的结果句子的概率值，来证实一下他们是不同的。

\subsubsection{Beam Search}
解决这个问题的一种方法是\textbf{Beam Search}。Beam Search和贪心搜索方法类似，但和贪心搜索在每个时刻只考虑一个最佳翻译假设不同，beam search在每个时刻考虑$b$个最佳翻译假设，这里$b$是beam的“深度”。在图23中给出了一个$b=2$时的beam search过程。
在第一个时刻，我们用词表中的这三个词来扩展$e_1$的翻译假设，然后保留得分排在前两个的候选（“b”和“a”）而删除剩下的那一个（“</s>”）。在第二个时刻，我们继续用词表中的词来扩展$e_2$的翻译假设，临时性生成$b * |V|$个假设。这些假设也会被剪枝到剩下$b$个有效假设（“ab”和“bb”）。先展开$b * |V|$个假设并计算其得分，然后剪枝到得分排在前$b$个的候选假设，这个操作过程会一直持续到整个句子生成结束为止。

在使用神经网络机器翻译模型等模型来生成句子时，需要注意一点，根据概率$P(E|F) = \prod_t^{|E|}P(e_t | F,e_1^{t-1})$来看，模型会倾向于生成短句。这是因为，每次我们给生成结果中加入另一个词时，其概率值都会被乘以一个概率值，从而降低了整个句子的概率值。当我们加大beam大小时，搜索算法会更容易找到这些短句，因此，beam大小较大的beam search算法对这些更短的句子有显著的\textbf{length bias}。

很多方法都在尝试解决这个长度偏执问题。例如，可以根据给定的源语言句子长度来设置一个生成句子长度的先验概率$P(|E| | |F|)$，然后在解码阶段，将这个概率和标准的句子概率$P(E | F)$相乘而得到：
\[
 \hat{E} = \arg \max \limits_{E} \log P(|E|||F|) + \log P(E|F).
\]
这个先验概率可以在训练数据中统计得到，文献【】通过在训练语料上统计到的多项分布来作为这个先验概率的估计值：
\[
 P(|E|||F|) = \frac{c(|E|,|F|)}{c(|F|)}.
\]
一个更具启发性但仍广泛被使用的方法，是用目标句子的长度来对其对数概率值进行归一化处理，有效地搜索平均每个词的概率对数最高的译文：
\[
 \hat{E} = \arg \max \limits_{E} \log P(E|F)/|E|.
\]

\subsection{序列编码的其他方法}
在7.1小节，我们描述了一个模型，这个模型从左到右一个词一个词地将这个线性序列编码。然而，这可能不是将句子$F$转换成向量$\textbf{h}$的最自然或最有效的方式。在本节内容中，我们将讨论一系列不同的编码方法，这些方法都曾在文献中被证明是有效的。

\subsubsection{倒序和双向编码}
首先，文献【】提出了\textbf{逆向编码器}。在这个方法中，我们让句子$F$通过一个标准的线性编码器，这次是从右到左的顺序来编码，而不是原来那样从左到右进行编码。
\[
 \overleftarrow{h}_t^{(f)} = \left \{ \begin{array}{ll}
  \overleftarrow{RNN}^{(f)}(\textbf{m}_t^{(f)},\textbf{\overleftarrow{h}}_{t+1}^{(f)}) & t \leq |F| \\
  \textbf{0} & \textrm{otherwise.}
 \end{array} \right.
\]

这里有个图



\subsubsection{卷积神经网络}


\subsubsection{树结构神经网络}


\subsection{多个模型的融合}


%\subsection{练习}

\section{注意力神经网络机器翻译}

\subsection{编码-解码模型中特征表示的问题}



\end{document}

